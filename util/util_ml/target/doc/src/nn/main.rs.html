<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `src\main.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>main.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../normalize.css"><link rel="stylesheet" type="text/css" href="../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../dark.css"><link rel="stylesheet" type="text/css" href="../../light.css" id="themeStyle"><script src="../../storage.js"></script><noscript><link rel="stylesheet" href="../../noscript.css"></noscript><link rel="shortcut icon" href="../../favicon.ico"><style type="text/css">#crate-search{background-image:url("../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../nn/index.html'><div class='logo-container'><img src='../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../theme.js"></script><nav class="sub"><form class="search-form"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" disabled autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><a id="settings-menu" href="../../settings.html"><img src="../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="comment">/*
To have a matrix_library and neural_network_library of all the functions related to nn in one palce
Derived from nnfs
May turn into a crate with generics
*/</span>
<span class="kw">mod</span> <span class="ident">matrix_lib</span>;
<span class="kw">mod</span> <span class="ident">nn_lib</span>;
<span class="kw">use</span> <span class="ident">nn_lib</span>::<span class="ident">LayerDetails</span>;
<span class="kw">mod</span> <span class="ident">ml_lib</span>;

<span class="kw">use</span> <span class="ident">rand</span>::<span class="kw-2">*</span>;

<span class="kw">fn</span> <span class="ident">main</span>() {
    <span class="comment">// matrix_lib.rs</span>

    <span class="kw">let</span> <span class="ident">v1</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">5.</span>, <span class="number">8.</span>, <span class="number">3.</span>, <span class="number">7.</span>, <span class="number">5.</span>, <span class="op">-</span><span class="number">2.</span>, <span class="op">-</span><span class="number">3.</span>, <span class="op">-</span><span class="number">5.</span>];
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">v2</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">99</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="op">-</span><span class="number">12</span>, <span class="op">-</span><span class="number">34</span>];

    <span class="comment">// vector addition check</span>
    <span class="kw">let</span> <span class="ident">v1_plus_v2</span> <span class="op">=</span>
        <span class="ident">matrix_lib</span>::<span class="ident">vector_addition</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">v1</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">x</span><span class="op">|</span> <span class="kw-2">*</span><span class="ident">x</span> <span class="kw">as</span> <span class="ident">i32</span>).<span class="ident">collect</span>(), <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">v2</span>);
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;The addition of of {:?} with {:?} is: {:?}&quot;</span>,
        <span class="ident">v1</span>, <span class="ident">v2</span>, <span class="ident">v1_plus_v2</span>
    );

    <span class="comment">// elemnt wise multiplicaiton check</span>
    <span class="kw">let</span> <span class="ident">v1_into_v2</span> <span class="op">=</span> <span class="ident">matrix_lib</span>::<span class="ident">element_wise_multiplication</span>(
        <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">v1</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">x</span><span class="op">|</span> <span class="kw-2">*</span><span class="ident">x</span> <span class="kw">as</span> <span class="ident">i32</span>).<span class="ident">collect</span>(),
        <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">v2</span>,
    );
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;The multiplication of {:?} with {:?} is: {:?}&quot;</span>,
        <span class="ident">v1</span>, <span class="ident">v2</span>, <span class="ident">v1_into_v2</span>
    );

    <span class="comment">// Shape changer</span>
    <span class="kw">let</span> <span class="ident">v2</span> <span class="op">=</span> <span class="ident">matrix_lib</span>::<span class="ident">shape_changer</span>(<span class="kw-2">&amp;</span><span class="ident">v2</span>, <span class="number">4</span>, <span class="number">3</span>);
    <span class="ident">matrix_lib</span>::<span class="ident">print_a_matrix</span>(<span class="string">&quot;\n2x5 version is:&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v2</span>);

    <span class="comment">// matrix transpose check</span>
    <span class="kw">let</span> <span class="ident">v2_t</span> <span class="op">=</span> <span class="ident">matrix_lib</span>::<span class="ident">transpose</span>(<span class="kw-2">&amp;</span><span class="ident">v2</span>);
    <span class="ident">matrix_lib</span>::<span class="ident">print_a_matrix</span>(<span class="string">&quot;The previous matrix is transposed to&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v2_t</span>);

    <span class="comment">// matrix multiplcation and dot product</span>
    <span class="kw">let</span> <span class="ident">v3</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="macro">vec</span><span class="macro">!</span>[<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>], <span class="macro">vec</span><span class="macro">!</span>[<span class="number">5</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="macro">vec</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">1</span>, <span class="number">6</span>]];
    <span class="kw">let</span> <span class="ident">v4</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="macro">vec</span><span class="macro">!</span>[<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="macro">vec</span><span class="macro">!</span>[<span class="number">5</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">1</span>], <span class="macro">vec</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">0</span>]];
    <span class="kw">let</span> <span class="ident">v3_v4</span> <span class="op">=</span> <span class="ident">matrix_lib</span>::<span class="ident">matrix_product</span>(<span class="kw-2">&amp;</span><span class="ident">v3</span>, <span class="kw-2">&amp;</span><span class="ident">v4</span>);
    <span class="ident">matrix_lib</span>::<span class="ident">print_a_matrix</span>(
        <span class="kw-2">&amp;</span><span class="macro">format</span><span class="macro">!</span>(<span class="string">&quot;The multiplicaiton of {:?} and {:?}&quot;</span>, <span class="ident">v3</span>, <span class="ident">v4</span>),
        <span class="kw-2">&amp;</span><span class="ident">v3_v4</span>,
    );

    <span class="comment">//================================================================================================================</span>
    <span class="ident">section_break</span>(<span class="string">&quot;MATRIX OVER&quot;</span>); <span class="comment">// nn_lib.rs</span>
                                  <span class="comment">// takes in only f64</span>

    <span class="comment">// ACTIVATION FUCNTION ReLU</span>
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;ReLU of {:?} is {:?}&quot;</span>, <span class="ident">v1</span>, <span class="ident">nn_lib</span>::<span class="ident">activation_relu</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>));

    <span class="comment">// ACTIVATION FUNCTION Leaky ReLU</span>
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Leaky ReLU of {:?} with alpha 0.1 is {:?}&quot;</span>,
        <span class="ident">v1</span>,
        <span class="ident">nn_lib</span>::<span class="ident">activation_leaky_relu</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="number">0.1</span>)
    );

    <span class="comment">// ACTIVATION FUNCTION Sigmoid</span>
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Sigmoid of {:?} is {:?}&quot;</span>,
        <span class="ident">v1</span>,
        <span class="ident">nn_lib</span>::<span class="ident">activation_sigmoid</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>)
    );

    <span class="comment">// ACTIVATION FUNCTION TanH</span>
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;TanH of {:?} is {:?}&quot;</span>, <span class="ident">v1</span>, <span class="ident">nn_lib</span>::<span class="ident">activation_tanh</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>));

    <span class="comment">// Creating neuron</span>
    <span class="kw">let</span> <span class="ident">n_features</span> <span class="op">=</span> <span class="number">6</span>;
    <span class="kw">let</span> <span class="ident">layer_1</span> <span class="op">=</span> <span class="ident">LayerDetails</span> {
        <span class="ident">n_inputs</span>: <span class="ident">n_features</span>,
        <span class="ident">n_neurons</span>: <span class="number">5</span>, <span class="comment">// can be of any size, depends</span>
    };

    <span class="comment">// Creating input</span>
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">input</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[];
    <span class="kw">let</span> <span class="ident">data_points</span> <span class="op">=</span> <span class="number">10</span>;
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">rng</span> <span class="op">=</span> <span class="ident">thread_rng</span>();
    <span class="kw">for</span> <span class="kw">_</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">data_points</span> {
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">row</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[];
        <span class="kw">for</span> <span class="kw">_</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">layer_1</span>.<span class="ident">n_inputs</span> {
            <span class="ident">row</span>.<span class="ident">push</span>(<span class="ident">rng</span>.<span class="ident">gen_range</span>(<span class="op">-</span><span class="number">10.</span>, <span class="number">10.</span>));
        }
        <span class="ident">input</span>.<span class="ident">push</span>(<span class="ident">row</span>);
    }
    <span class="ident">matrix_lib</span>::<span class="ident">print_a_matrix</span>(<span class="string">&quot;\nInput generated is :&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">input</span>);

    <span class="comment">// Output of a layer activation_function(input*weights+bias)</span>
    <span class="kw">let</span> <span class="ident">output</span> <span class="op">=</span> <span class="ident">layer_1</span>.<span class="ident">output_of_layer</span>(
        <span class="kw-2">&amp;</span><span class="ident">input</span>,
        <span class="kw-2">&amp;</span><span class="ident">layer_1</span>.<span class="ident">create_weights</span>(),
        <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">layer_1</span>.<span class="ident">create_bias</span>(),
        <span class="ident">nn_lib</span>::<span class="ident">activation_relu</span>, <span class="comment">// to be choosen by user, when there are other fucntiosn</span>
    );

    <span class="ident">matrix_lib</span>::<span class="ident">print_a_matrix</span>(<span class="string">&quot;\nOutput generated is :&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">output</span>);

    <span class="comment">//================================================================================================================</span>
    <span class="ident">section_break</span>(<span class="string">&quot;NN OVER&quot;</span>);
    <span class="comment">// ml_lib.rs</span>
    <span class="kw">let</span> <span class="ident">v1</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">3.</span>, <span class="number">5.</span>];
    <span class="kw">let</span> <span class="ident">v2</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">5.</span>];
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;Mean of {:?} is {}&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="ident">ml_lib</span>::<span class="ident">mean</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>));
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;variance of {:?} is {}&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="ident">ml_lib</span>::<span class="ident">variance</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>));
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;Mean of {:?} is {}&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="ident">ml_lib</span>::<span class="ident">mean</span>(<span class="kw-2">&amp;</span><span class="ident">v2</span>));
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;variance of {:?} is {}&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="ident">ml_lib</span>::<span class="ident">variance</span>(<span class="kw-2">&amp;</span><span class="ident">v2</span>));
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;The covariance of {:?} and {:?} is {}&quot;</span>,
        <span class="kw-2">&amp;</span><span class="ident">v1</span>,
        <span class="kw-2">&amp;</span><span class="ident">v2</span>,
        <span class="ident">ml_lib</span>::<span class="ident">covariance</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="kw-2">&amp;</span><span class="ident">v2</span>)
    );
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Coefficient of {:?} and {:?} are b0 = {} and b1 = {}&quot;</span>,
        <span class="kw-2">&amp;</span><span class="ident">v1</span>,
        <span class="kw-2">&amp;</span><span class="ident">v2</span>,
        <span class="ident">ml_lib</span>::<span class="ident">coefficient</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="kw-2">&amp;</span><span class="ident">v2</span>).<span class="number">0</span>,
        <span class="ident">ml_lib</span>::<span class="ident">coefficient</span>(<span class="kw-2">&amp;</span><span class="ident">v1</span>, <span class="kw-2">&amp;</span><span class="ident">v2</span>).<span class="number">1</span>
    );

    <span class="comment">// Simple linear regression</span>
    <span class="kw">let</span> <span class="ident">to_train_on</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[
        (<span class="number">1.</span>, <span class="number">2.</span>),
        (<span class="number">2.</span>, <span class="number">3.</span>),
        (<span class="number">4.</span>, <span class="number">5.</span>),
        (<span class="number">3.</span>, <span class="number">5.</span>),
        (<span class="number">6.</span>, <span class="number">8.</span>),
        (<span class="number">7.</span>, <span class="number">8.</span>),
        (<span class="number">9.</span>, <span class="number">10.</span>),
        (<span class="number">1.</span>, <span class="number">2.5</span>),
        (<span class="number">11.</span>, <span class="number">12.</span>),
        (<span class="number">5.</span>, <span class="number">4.</span>),
        (<span class="number">7.</span>, <span class="number">7.</span>),
        (<span class="number">6.</span>, <span class="number">6.</span>),
        (<span class="number">8.</span>, <span class="number">9.</span>),
    ];
    <span class="kw">let</span> <span class="ident">to_test_on</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[(<span class="number">10.</span>, <span class="number">11.</span>), (<span class="number">9.</span>, <span class="number">12.</span>), (<span class="number">11.</span>, <span class="number">12.5</span>)];
    <span class="kw">let</span> <span class="ident">predicted_output</span> <span class="op">=</span> <span class="ident">ml_lib</span>::<span class="ident">simple_linear_regression_prediction</span>(<span class="kw-2">&amp;</span><span class="ident">to_train_on</span>, <span class="kw-2">&amp;</span><span class="ident">to_test_on</span>);
    <span class="kw">let</span> <span class="ident">original_output</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">to_test_on</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">a</span><span class="op">|</span> <span class="ident">a</span>.<span class="number">0</span>).<span class="ident">collect</span>();
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Predicted is {:?}\nOriginal is {:?}&quot;</span>,
        <span class="kw-2">&amp;</span><span class="ident">predicted_output</span>, <span class="kw-2">&amp;</span><span class="ident">original_output</span>
    );

    <span class="comment">// reading in a file to have table</span>
    <span class="kw">let</span> <span class="ident">df</span> <span class="op">=</span> <span class="ident">ml_lib</span>::<span class="ident">read_csv</span>(<span class="string">&quot;./data/dataset_iris.txt&quot;</span>.<span class="ident">to_string</span>(), <span class="number">5</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;{:?}&quot;</span>, <span class="ident">df</span>.<span class="ident">values</span>());

    <span class="comment">// unique values</span>
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Unique classes are {:?}&quot;</span>,
        <span class="ident">ml_lib</span>::<span class="ident">unique_values</span>(<span class="kw-2">&amp;</span><span class="ident">df</span>[<span class="string">&quot;species&quot;</span>].<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">a</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="ident">a</span>).<span class="ident">collect</span>()) <span class="comment">// converting String to &amp;str as copy is not implemented for String</span>
    );

    <span class="comment">// type conversion and missing value replacement</span>
    <span class="kw">let</span> <span class="ident">conversion</span> <span class="op">=</span> <span class="ident">ml_lib</span>::<span class="ident">convert_and_impute</span>(<span class="kw-2">&amp;</span><span class="ident">df</span>[<span class="string">&quot;petal_length&quot;</span>], <span class="number">0.</span>, <span class="number">999.</span>);
    <span class="kw">let</span> <span class="ident">floating_petal_length</span> <span class="op">=</span> <span class="ident">conversion</span>.<span class="number">0</span>.<span class="ident">unwrap</span>();
    <span class="kw">let</span> <span class="ident">missing_value</span> <span class="op">=</span> <span class="ident">conversion</span>.<span class="number">1</span>;
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;{:?}\nis now\n{:?}\nwith missing values at\n{:?}&quot;</span>,
        <span class="ident">df</span>[<span class="string">&quot;petal_length&quot;</span>], <span class="ident">floating_petal_length</span>, <span class="ident">missing_value</span>
    );

    <span class="comment">// missing string imputation</span>
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">species</span> <span class="op">=</span> <span class="ident">df</span>[<span class="string">&quot;species&quot;</span>].<span class="ident">clone</span>();
    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;{:?}\nis now\n{:?}&quot;</span>,
        <span class="kw-2">&amp;</span><span class="ident">df</span>[<span class="string">&quot;species&quot;</span>],
        <span class="ident">ml_lib</span>::<span class="ident">impute_string</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">species</span>, <span class="string">&quot;UNKNOWN&quot;</span>)
    );

    <span class="macro">println</span><span class="macro">!</span>(
        <span class="string">&quot;Now the unique classes are {:?}&quot;</span>,
        <span class="ident">ml_lib</span>::<span class="ident">unique_values</span>(<span class="kw-2">&amp;</span><span class="ident">ml_lib</span>::<span class="ident">impute_string</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">species</span>, <span class="string">&quot;UNKNOWN&quot;</span>)) <span class="comment">// converting String to &amp;str as copy is not implemented for String</span>
    );
    <span class="comment">//================================================================================================================</span>
    <span class="ident">section_break</span>(<span class="string">&quot;ML OVER&quot;</span>);
}

<span class="comment">// fn main() {</span>
<span class="comment">//     let v1 = vec![1., 2., 5., 0.8, 3., 7., 5., -2., -3., -5.];</span>
<span class="comment">//     println!(&quot;Tanh of {:?} is {:?}&quot;, v1, nn_lib::activation_tanh(&amp;v1));</span>
<span class="comment">// }</span>
<span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">section_break</span>(<span class="ident">display</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>) {
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;&quot;</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;&quot;</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;========================================================================================================================================================================&quot;</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;                                          {:?}&quot;</span>, <span class="ident">display</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;========================================================================================================================================================================&quot;</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;&quot;</span>);
    <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;&quot;</span>);
}

<span class="comment">/* OUTPUT
The changed vector is [1, 2, 5, 8, 3, 7, 5, -2, -3, -5, 0, 0]
The addition of of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] with [1, 6, 2, 7, 3, 7, 99, 6, 3, 6, -12, -34] is: [2, 8, 7, 15, 6, 14, 104, 4, 0, 1]
The changed vector is [1, 2, 5, 8, 3, 7, 5, -2, -3, -5, 0, 0]
The multiplication of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] with [1, 6, 2, 7, 3, 7, 99, 6, 3, 6, -12, -34] is: [1, 12, 10, 56, 9, 49, 495, -12, -9, -30, 0, 0]

2x5 version is:
[1, 6, 2, 7]
[3, 7, 99, 6]
[3, 6, -12, -34]


The previous matrix is transposed to
[1, 3, 3]
[6, 7, 6]
[2, 99, -12]
[7, 6, -34]


Multiplication of 3x3 and 3x4
Output will be 3x4
The multiplicaiton of [[1, 4, 4], [5, 8, 9], [0, 1, 6]] and [[1, 4, 4, 5], [5, 8, 9, 1], [0, 1, 6, 0]]
[21, 40, 64]
[9, 45, 93]
[146, 33, 5]
[14, 45, 1]




========================================================================================================================================================================                                          &quot;MATRIX OVER&quot;
========================================================================================================================================================================

ReLU of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] is [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, 0.0, 0.0, 0.0]
Leaky ReLU of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] with alpha 0.1 is [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -0.2, -0.30000000000000004, -0.5]
Sigmoid of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] is [0.2689414213699951, 0.11920292202211755, 0.0066928509242848554, 0.0003353501304664781, 0.04742587317756678, 0.0009110511944006454, 0.0066928509242848554, 0.8807970779778823, 0.9525741268224334, 0.9933071490757153]
TanH of [1.0, 2.0, 5.0, 8.0, 3.0, 7.0, 5.0, -2.0, -3.0, -5.0] is [0.7615941559557649, 0.964027580075817, 0.999909204262595, 0.999999774929676, 0.9950547536867306, 0.9999983369439447, 0.999909204262595, -0.964027580075817, -0.9950547536867306, -0.999909204262595]

Input generated is :
[-3.837912522679594, -8.190384763576128, -7.003712991612043, -6.13854186649772, 0.20482280769636318, 2.4363752291148266]
[4.2360375994915245, 5.223692299206348, 5.833230398457893, -6.507009121674474, -3.0209475882876102, -0.23431339472820056]
[9.253333002337559, 1.0256507482291433, -2.4731822989589425, 3.2127814405002653, 4.633122265963795, 2.5260191049033267]
[5.082507804239, -6.310542398329413, -5.663522312845561, -7.954469158627462, -3.574067006027768, 1.751943478666016]
[9.789951170465393, 1.0199579496846845, 9.430031151355887, 6.230989090731619, 6.80640518834776, -7.804860154208484]
[-8.208053177214678, -0.26359996630296223, 6.562485430597086, -0.045740397712279446, 5.585463553796357, -2.2294681618247614]
[4.735671243660505, 8.903501382801686, -9.822867188118508, -3.233726507326131, -5.911980859224171, 8.264786695548196]
[-1.359226776435456, -6.2491952509150295, -0.4707814910112553, -2.832951359469278, -9.643691772515446, 9.348129045642565]
[-6.3630284074617816, -0.912555906969823, 7.842501514190214, -3.6428469093209648, -3.632472216030007, -3.3712121764904035]
[-6.543864899662681, -7.333521696223939, 9.85251637484312, -5.625167099176394, -0.06838304168239162, -0.3401345276847323]


Multiplication of 10x6 and 6x5
Output will be 10x5

Output generated is :
[1.1193253793971212, 6.348298115173648, 11.643857124692977, 0.0, 0.0]
[0.0, 3.2161092243527185, 5.006731631858815, 3.4619052246112747, 0.025720869608846897]
[0.4681120881817362, 0.0, 0.0, 2.5767045563720297, 0.0]
[0.0, 0.0, 1.3235943647450146, 3.857281629077436, 0.0]
[7.513175453425225, 0.0, 0.0, 6.173185713348137, 1.6114689276880234]
[0.0, 1.2624206758489196, 0.0, 0.0, 1.1358197478484535]
[7.505107963802638, 0.0, 6.520602909691198, 0.0, 0.06113838441102579]
[0.0, 0.0, 0.0, 0.0, 0.0]
[3.825979231769935, 0.0, 3.96611563704401, 0.0, 0.0]
[0.0, 2.3062054749468675, 2.45873067673489, 0.0, 0.2492175050857273]




========================================================================================================================================================================                                          &quot;NN OVER&quot;
========================================================================================================================================================================

Mean of [1.0, 2.0, 4.0, 3.0, 5.0] is 3
variance of [1.0, 2.0, 4.0, 3.0, 5.0] is 10
Mean of [1.0, 2.0, 4.0, 3.0, 5.0] is 2.8
variance of [1.0, 2.0, 4.0, 3.0, 5.0] is 8.8
The covariance of [1.0, 2.0, 4.0, 3.0, 5.0] and [1.0, 3.0, 3.0, 2.0, 5.0] is 8
Coefficient of [1.0, 2.0, 4.0, 3.0, 5.0] and [1.0, 3.0, 3.0, 2.0, 5.0] are b0 = 0.39999999999999947 and b1 = 0.8
========================================================================================================================================================
RMSE: 2.080646271630258
Predicted is [11.92023172905526, 11.901403743315509, 11.93905971479501]
Original is [10.0, 9.0, 11.0]
========================================================================================================================================================
Reading the file ...
Input row count is 30
The header is [&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;]
[[&quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.4&quot;, &quot;0.3&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.1&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.1&quot;, &quot;0.1&quot;, &quot;0.2&quot;, &quot;0.4&quot;, &quot;0.4&quot;, &quot;0.3&quot;, &quot;0.3&quot;, &quot;0.3&quot;, &quot;0.2&quot;, &quot;0.4&quot;, &quot;0.2&quot;, &quot;0.5&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.4&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.4&quot;, &quot;0.1&quot;, &quot;0.2&quot;, &quot;0.1&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.1&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.3&quot;, &quot;0.3&quot;, &quot;0.2&quot;, &quot;0.6&quot;, &quot;0.4&quot;, &quot;0.3&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;0.2&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.6&quot;, &quot;1.0&quot;, &quot;1.3&quot;, &quot;1.4&quot;, &quot;1.0&quot;, &quot;1.5&quot;, &quot;1.0&quot;, &quot;1.4&quot;, &quot;1.3&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.0&quot;, &quot;1.5&quot;, &quot;1.1&quot;, &quot;1.8&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.2&quot;, &quot;1.3&quot;, &quot;1.4&quot;, &quot;1.4&quot;, &quot;1.7&quot;, &quot;1.5&quot;, &quot;1.0&quot;, &quot;1.1&quot;, &quot;1.0&quot;, &quot;1.2&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.2&quot;, &quot;1.4&quot;, &quot;1.2&quot;, &quot;1.0&quot;, &quot;1.3&quot;, &quot;1.2&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.1&quot;, &quot;1.3&quot;, &quot;2.5&quot;, &quot;1.9&quot;, &quot;2.1&quot;, &quot;1.8&quot;, &quot;2.2&quot;, &quot;2.1&quot;, &quot;1.7&quot;, &quot;1.8&quot;, &quot;1.8&quot;, &quot;2.5&quot;, &quot;2.0&quot;, &quot;1.9&quot;, &quot;2.1&quot;, &quot;2.0&quot;, &quot;2.4&quot;, &quot;2.3&quot;, &quot;1.8&quot;, &quot;2.2&quot;, &quot;2.3&quot;, &quot;1.5&quot;, &quot;2.3&quot;, &quot;2.0&quot;, &quot;2.0&quot;, &quot;1.8&quot;, &quot;2.1&quot;, &quot;1.8&quot;, &quot;1.8&quot;, &quot;1.8&quot;, &quot;2.1&quot;, &quot;1.6&quot;, &quot;1.9&quot;, &quot;2.0&quot;, &quot;2.2&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;2.3&quot;, &quot;2.4&quot;, &quot;1.8&quot;, &quot;1.8&quot;, &quot;2.1&quot;, &quot;2.4&quot;, &quot;2.3&quot;, &quot;1.9&quot;, &quot;2.3&quot;, &quot;2.5&quot;, &quot;2.3&quot;, &quot;1.9&quot;, &quot;2.0&quot;, &quot;2.3&quot;, &quot;1.8&quot;], [&quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;,
&quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;], [&quot;1.4&quot;, &quot;1.4&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.5&quot;, &quot;1.6&quot;, &quot;1.4&quot;, &quot;1.1&quot;, &quot;1.2&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.4&quot;, &quot;1.7&quot;, &quot;1.5&quot;, &quot;1.7&quot;, &quot;1.5&quot;, &quot;1.0&quot;, &quot;1.7&quot;, &quot;1.9&quot;, &quot;1.6&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.6&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.2&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.6&quot;, &quot;1.9&quot;, &quot;1.4&quot;, &quot;1.6&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;4.7&quot;, &quot;4.5&quot;, &quot;4.9&quot;, &quot;4.0&quot;, &quot;4.6&quot;, &quot;4.5&quot;, &quot;4.7&quot;, &quot;3.3&quot;, &quot;4.6&quot;, &quot;3.9&quot;, &quot;3.5&quot;, &quot;4.2&quot;, &quot;4.0&quot;, &quot;4.7&quot;, &quot;3.6&quot;, &quot;4.4&quot;, &quot;4.5&quot;, &quot;4.1&quot;, &quot;4.5&quot;, &quot;3.9&quot;, &quot;4.8&quot;, &quot;4.0&quot;, &quot;4.9&quot;, &quot;4.7&quot;, &quot;4.3&quot;, &quot;4.4&quot;, &quot;4.8&quot;, &quot;5.0&quot;, &quot;4.5&quot;, &quot;3.5&quot;, &quot;3.8&quot;, &quot;3.7&quot;, &quot;3.9&quot;, &quot;5.1&quot;, &quot;4.5&quot;, &quot;4.5&quot;, &quot;4.7&quot;, &quot;4.4&quot;, &quot;4.1&quot;, &quot;4.0&quot;, &quot;4.4&quot;, &quot;4.6&quot;, &quot;4.0&quot;, &quot;3.3&quot;, &quot;4.2&quot;, &quot;4.2&quot;, &quot;4.2&quot;, &quot;4.3&quot;, &quot;3.0&quot;, &quot;4.1&quot;, &quot;6.0&quot;, &quot;5.1&quot;, &quot;5.9&quot;, &quot;5.6&quot;, &quot;5.8&quot;, &quot;6.6&quot;, &quot;4.5&quot;, &quot;6.3&quot;, &quot;5.8&quot;, &quot;6.1&quot;, &quot;5.1&quot;, &quot;5.3&quot;, &quot;5.5&quot;, &quot;5.0&quot;, &quot;5.1&quot;, &quot;5.3&quot;, &quot;5.5&quot;, &quot;6.7&quot;, &quot;6.9&quot;, &quot;5.0&quot;, &quot;5.7&quot;, &quot;4.9&quot;, &quot;6.7&quot;, &quot;4.9&quot;, &quot;5.7&quot;, &quot;6.0&quot;, &quot;4.8&quot;, &quot;4.9&quot;, &quot;5.6&quot;, &quot;5.8&quot;, &quot;6.1&quot;, &quot;6.4&quot;, &quot;5.6&quot;, &quot;5.1&quot;, &quot;5.6&quot;, &quot;6.1&quot;, &quot;5.6&quot;, &quot;5.5&quot;, &quot;4.8&quot;, &quot;5.4&quot;, &quot;5.6&quot;, &quot;5.1&quot;, &quot;5.1&quot;, &quot;5.9&quot;, &quot;5.7&quot;, &quot;5.2&quot;, &quot;5.0&quot;, &quot;5.2&quot;, &quot;5.4&quot;, &quot;5.1&quot;], [&quot;5.1&quot;, &quot;4.9&quot;, &quot;4.7&quot;, &quot;4.6&quot;, &quot;5.0&quot;, &quot;5.4&quot;, &quot;4.6&quot;, &quot;5.0&quot;, &quot;4.4&quot;, &quot;4.9&quot;, &quot;5.4&quot;, &quot;4.8&quot;, &quot;4.8&quot;, &quot;4.3&quot;, &quot;5.8&quot;, &quot;5.7&quot;, &quot;5.4&quot;, &quot;5.1&quot;, &quot;5.7&quot;, &quot;5.1&quot;, &quot;5.4&quot;, &quot;5.1&quot;, &quot;4.6&quot;, &quot;5.1&quot;, &quot;4.8&quot;, &quot;5.0&quot;, &quot;5.0&quot;, &quot;5.2&quot;, &quot;5.2&quot;, &quot;4.7&quot;, &quot;4.8&quot;, &quot;5.4&quot;, &quot;5.2&quot;, &quot;5.5&quot;, &quot;4.9&quot;, &quot;5.0&quot;, &quot;5.5&quot;, &quot;4.9&quot;, &quot;4.4&quot;, &quot;5.1&quot;, &quot;5.0&quot;, &quot;4.5&quot;, &quot;4.4&quot;, &quot;5.0&quot;, &quot;5.1&quot;, &quot;4.8&quot;, &quot;5.1&quot;, &quot;4.6&quot;, &quot;5.3&quot;, &quot;5.0&quot;, &quot;7.0&quot;, &quot;6.4&quot;, &quot;6.9&quot;, &quot;5.5&quot;, &quot;6.5&quot;, &quot;5.7&quot;, &quot;6.3&quot;, &quot;4.9&quot;, &quot;6.6&quot;, &quot;5.2&quot;, &quot;5.0&quot;, &quot;5.9&quot;, &quot;6.0&quot;, &quot;6.1&quot;, &quot;5.6&quot;, &quot;6.7&quot;, &quot;5.6&quot;, &quot;5.8&quot;, &quot;6.2&quot;, &quot;5.6&quot;, &quot;5.9&quot;, &quot;6.1&quot;, &quot;6.3&quot;, &quot;6.1&quot;, &quot;6.4&quot;, &quot;6.6&quot;, &quot;6.8&quot;, &quot;6.7&quot;, &quot;6.0&quot;, &quot;5.7&quot;, &quot;5.5&quot;, &quot;5.5&quot;, &quot;5.8&quot;, &quot;6.0&quot;, &quot;5.4&quot;, &quot;6.0&quot;, &quot;6.7&quot;, &quot;6.3&quot;, &quot;5.6&quot;, &quot;5.5&quot;, &quot;5.5&quot;, &quot;6.1&quot;, &quot;5.8&quot;, &quot;5.0&quot;, &quot;5.6&quot;, &quot;5.7&quot;, &quot;5.7&quot;, &quot;6.2&quot;, &quot;5.1&quot;, &quot;5.7&quot;, &quot;6.3&quot;, &quot;5.8&quot;, &quot;7.1&quot;, &quot;6.3&quot;, &quot;6.5&quot;, &quot;7.6&quot;, &quot;4.9&quot;, &quot;7.3&quot;, &quot;6.7&quot;, &quot;7.2&quot;, &quot;6.5&quot;, &quot;6.4&quot;, &quot;6.8&quot;, &quot;5.7&quot;, &quot;5.8&quot;, &quot;6.4&quot;, &quot;6.5&quot;, &quot;7.7&quot;, &quot;7.7&quot;, &quot;6.0&quot;, &quot;6.9&quot;, &quot;5.6&quot;, &quot;7.7&quot;, &quot;6.3&quot;, &quot;6.7&quot;, &quot;7.2&quot;, &quot;6.2&quot;, &quot;6.1&quot;, &quot;6.4&quot;, &quot;7.2&quot;, &quot;7.4&quot;, &quot;7.9&quot;, &quot;6.4&quot;, &quot;6.3&quot;, &quot;6.1&quot;, &quot;7.7&quot;, &quot;6.3&quot;, &quot;6.4&quot;, &quot;6.0&quot;, &quot;6.9&quot;, &quot;6.7&quot;, &quot;6.9&quot;, &quot;5.8&quot;, &quot;6.8&quot;, &quot;6.7&quot;, &quot;6.7&quot;, &quot;6.3&quot;, &quot;6.5&quot;, &quot;6.2&quot;, &quot;5.9&quot;], [&quot;3.5&quot;, &quot;3.0&quot;, &quot;3.2&quot;, &quot;3.1&quot;, &quot;3.6&quot;, &quot;3.9&quot;, &quot;3.4&quot;, &quot;3.4&quot;, &quot;2.9&quot;, &quot;3.1&quot;, &quot;3.7&quot;, &quot;3.4&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;4.0&quot;, &quot;4.4&quot;, &quot;3.9&quot;, &quot;3.5&quot;, &quot;3.8&quot;, &quot;3.8&quot;, &quot;3.4&quot;, &quot;3.7&quot;, &quot;3.6&quot;, &quot;3.3&quot;, &quot;3.4&quot;, &quot;3.0&quot;, &quot;3.4&quot;, &quot;3.5&quot;, &quot;3.4&quot;, &quot;3.2&quot;, &quot;3.1&quot;, &quot;3.4&quot;, &quot;4.1&quot;, &quot;4.2&quot;, &quot;3.1&quot;, &quot;3.2&quot;, &quot;3.5&quot;, &quot;3.1&quot;, &quot;3.0&quot;, &quot;3.4&quot;, &quot;3.5&quot;, &quot;2.3&quot;, &quot;3.2&quot;, &quot;3.5&quot;, &quot;3.8&quot;, &quot;3.0&quot;, &quot;3.8&quot;, &quot;3.2&quot;, &quot;3.7&quot;, &quot;3.3&quot;, &quot;3.2&quot;, &quot;3.2&quot;, &quot;3.1&quot;, &quot;2.3&quot;, &quot;2.8&quot;, &quot;2.8&quot;, &quot;3.3&quot;, &quot;2.4&quot;, &quot;2.9&quot;, &quot;2.7&quot;, &quot;2.0&quot;, &quot;3.0&quot;, &quot;2.2&quot;, &quot;2.9&quot;, &quot;2.9&quot;, &quot;3.1&quot;, &quot;3.0&quot;, &quot;2.7&quot;, &quot;2.2&quot;, &quot;2.5&quot;, &quot;3.2&quot;, &quot;2.8&quot;, &quot;2.5&quot;, &quot;2.8&quot;, &quot;2.9&quot;, &quot;3.0&quot;, &quot;2.8&quot;, &quot;3.0&quot;, &quot;2.9&quot;, &quot;2.6&quot;, &quot;2.4&quot;, &quot;2.4&quot;, &quot;2.7&quot;, &quot;2.7&quot;, &quot;3.0&quot;, &quot;3.4&quot;, &quot;3.1&quot;, &quot;2.3&quot;, &quot;3.0&quot;, &quot;2.5&quot;, &quot;2.6&quot;, &quot;3.0&quot;, &quot;2.6&quot;, &quot;2.3&quot;, &quot;2.7&quot;, &quot;3.0&quot;, &quot;2.9&quot;, &quot;2.9&quot;, &quot;2.5&quot;, &quot;2.8&quot;, &quot;3.3&quot;, &quot;2.7&quot;, &quot;3.0&quot;, &quot;2.9&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;2.5&quot;, &quot;2.9&quot;, &quot;2.5&quot;, &quot;3.6&quot;, &quot;3.2&quot;, &quot;2.7&quot;, &quot;3.0&quot;, &quot;2.5&quot;, &quot;2.8&quot;, &quot;3.2&quot;, &quot;3.0&quot;, &quot;3.8&quot;, &quot;2.6&quot;, &quot;2.2&quot;, &quot;3.2&quot;, &quot;2.8&quot;, &quot;2.8&quot;, &quot;2.7&quot;, &quot;3.3&quot;, &quot;3.2&quot;, &quot;2.8&quot;, &quot;3.0&quot;, &quot;2.8&quot;, &quot;3.0&quot;, &quot;2.8&quot;, &quot;3.8&quot;, &quot;2.8&quot;, &quot;2.8&quot;, &quot;2.6&quot;, &quot;3.0&quot;, &quot;3.4&quot;, &quot;3.1&quot;, &quot;3.0&quot;, &quot;3.1&quot;, &quot;3.1&quot;, &quot;3.1&quot;, &quot;2.7&quot;, &quot;3.2&quot;, &quot;3.3&quot;, &quot;3.0&quot;, &quot;2.5&quot;, &quot;3.0&quot;, &quot;3.4&quot;, &quot;3.0&quot;]]
========================================================================================================================================================
Unique classes are [&quot;setosa&quot;, &quot;&quot;, &quot;versicolor&quot;, &quot;virginica&quot;]
========================================================================================================================================================
Error found in 5th position of the vector
[&quot;1.4&quot;, &quot;1.4&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.5&quot;, &quot;1.6&quot;, &quot;1.4&quot;, &quot;1.1&quot;, &quot;1.2&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.4&quot;, &quot;1.7&quot;, &quot;1.5&quot;, &quot;1.7&quot;, &quot;1.5&quot;, &quot;1.0&quot;, &quot;1.7&quot;, &quot;1.9&quot;, &quot;1.6&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.6&quot;, &quot;1.6&quot;, &quot;1.5&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.2&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.5&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.3&quot;, &quot;1.6&quot;, &quot;1.9&quot;, &quot;1.4&quot;, &quot;1.6&quot;, &quot;1.4&quot;, &quot;1.5&quot;, &quot;1.4&quot;, &quot;4.7&quot;, &quot;4.5&quot;, &quot;4.9&quot;, &quot;4.0&quot;, &quot;4.6&quot;, &quot;4.5&quot;, &quot;4.7&quot;, &quot;3.3&quot;, &quot;4.6&quot;, &quot;3.9&quot;, &quot;3.5&quot;, &quot;4.2&quot;, &quot;4.0&quot;, &quot;4.7&quot;, &quot;3.6&quot;, &quot;4.4&quot;, &quot;4.5&quot;, &quot;4.1&quot;, &quot;4.5&quot;, &quot;3.9&quot;, &quot;4.8&quot;, &quot;4.0&quot;, &quot;4.9&quot;, &quot;4.7&quot;, &quot;4.3&quot;, &quot;4.4&quot;, &quot;4.8&quot;, &quot;5.0&quot;, &quot;4.5&quot;, &quot;3.5&quot;, &quot;3.8&quot;, &quot;3.7&quot;, &quot;3.9&quot;, &quot;5.1&quot;, &quot;4.5&quot;, &quot;4.5&quot;, &quot;4.7&quot;, &quot;4.4&quot;, &quot;4.1&quot;, &quot;4.0&quot;, &quot;4.4&quot;, &quot;4.6&quot;, &quot;4.0&quot;, &quot;3.3&quot;, &quot;4.2&quot;, &quot;4.2&quot;, &quot;4.2&quot;, &quot;4.3&quot;, &quot;3.0&quot;, &quot;4.1&quot;, &quot;6.0&quot;, &quot;5.1&quot;, &quot;5.9&quot;, &quot;5.6&quot;, &quot;5.8&quot;, &quot;6.6&quot;, &quot;4.5&quot;, &quot;6.3&quot;, &quot;5.8&quot;, &quot;6.1&quot;, &quot;5.1&quot;, &quot;5.3&quot;, &quot;5.5&quot;, &quot;5.0&quot;, &quot;5.1&quot;, &quot;5.3&quot;, &quot;5.5&quot;, &quot;6.7&quot;, &quot;6.9&quot;, &quot;5.0&quot;, &quot;5.7&quot;, &quot;4.9&quot;, &quot;6.7&quot;, &quot;4.9&quot;, &quot;5.7&quot;, &quot;6.0&quot;, &quot;4.8&quot;, &quot;4.9&quot;, &quot;5.6&quot;, &quot;5.8&quot;, &quot;6.1&quot;, &quot;6.4&quot;, &quot;5.6&quot;, &quot;5.1&quot;, &quot;5.6&quot;, &quot;6.1&quot;, &quot;5.6&quot;, &quot;5.5&quot;, &quot;4.8&quot;, &quot;5.4&quot;, &quot;5.6&quot;, &quot;5.1&quot;, &quot;5.1&quot;, &quot;5.9&quot;, &quot;5.7&quot;, &quot;5.2&quot;, &quot;5.0&quot;, &quot;5.2&quot;, &quot;5.4&quot;, &quot;5.1&quot;]
is now
[1.4, 1.4, 1.3, 1.5, 1.4, 999.0, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5,
1.4, 1.5, 1.2, 1.3, 1.5, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4, 4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1, 6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1]
with missing values at
[5]
========================================================================================================================================================
Missing value found in 11th position of the vector
[&quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;]
is now
[&quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;UNKNOWN&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;versicolor&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;, &quot;virginica&quot;]
========================================================================================================================================================
Missing value found in 11th position of the vector
========================================================================================================================================================
Now the unique classes are [&quot;setosa&quot;, &quot;UNKNOWN&quot;, &quot;versicolor&quot;, &quot;virginica&quot;]


========================================================================================================================================================================                                          &quot;ML OVER&quot;
========================================================================================================================================================================
*/</span>
</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><script>window.rootPath = "../../";window.currentCrate = "nn";</script><script src="../../aliases.js"></script><script src="../../main.js"></script><script src="../../source-script.js"></script><script src="../../source-files.js"></script><script defer src="../../search-index.js"></script></body></html>